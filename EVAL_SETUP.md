# ğŸ”¬ Evaluation System - Setup Guide

Quick guide to set up and use the OpenAI LLM-as-a-judge evaluation system.

## âœ… What You Have Now

A comprehensive evaluation system with:
- **OpenAI GPT-4** as an expert judge
- **Domain-specific** rule-based checks
- **Pipeline integration** for automated quality gates
- **A/B testing** framework
- **Pre-built test cases** for ticketing scenarios

---

## ğŸš€ Setup (2 Minutes)

### Step 1: Install Dependencies
```bash
pip install openai>=1.0.0
```

### Step 2: Set Your OpenAI API Key
```bash
# Add to your environment
export OPENAI_API_KEY=sk-your-actual-openai-api-key-here

# Or add to .env file
echo "OPENAI_API_KEY=sk-your-actual-openai-api-key-here" >> .env
```

### Step 3: Run Evaluation
```bash
cd evals
python3 eval_suite.py
```

---

## ğŸ“Š What Happens When You Run It

### The eval suite will:

1. **Initialize** both evaluators (GPT-4 + domain checks)
2. **Run 9 test cases** covering:
   - Simple purchases
   - Seat upgrades
   - Price inquiries
   - Complex requests
   - Refunds

3. **Evaluate each response** on:
   - Helpfulness (is it useful?)
   - Accuracy (is it correct?)
   - Appropriateness (professional tone?)
   - Tool usage (right tools?)
   - Conversation flow (natural?)
   - Domain expertise (ticketing knowledge?)
   - PLUS: Price math, inventory validation, policy compliance

4. **Generate report** showing:
   ```
   âœ… Passed: 9/9 (100%)
   ğŸ“Š Avg score: 0.89/1.00
   ğŸ’¡ Recommendation: READY FOR PRODUCTION
   ```

5. **Save results** to `eval_results.json`

---

## ğŸ’¡ Example Output

```
ğŸ”¬ COMPREHENSIVE EVALUATION SUITE
================================================================================

âœ… Configuration:
   LLM Judge: Enabled
   Domain Checks: Enabled

ğŸ“‹ Evaluating 9 test cases...

Test 1/9: Simple ticket purchase
  Score: 0.92 âœ…
  Feedback: Excellent response with clear pricing...

Test 2/9: Seat upgrade request
  Score: 0.88 âœ…
  Feedback: Good approach, could be more specific...

...

================================================================================
ğŸ“Š EVALUATION REPORT
================================================================================

âœ… Summary:
   Total cases: 9
   Passed: 9
   Failed: 0
   Pass rate: 100.0%
   Avg score: 0.89

ğŸ¤– LLM Judge Stats:
   avg_helpfulness     : 0.92
   avg_accuracy        : 0.88
   avg_appropriateness : 0.90
   avg_tool_usage      : 0.85
   avg_conversation_flow: 0.89
   avg_domain_expertise : 0.84

ğŸ¯ Domain Check Stats:
   total_checks        : 45
   checks_passed       : 43
   checks_failed       : 2
   check_pass_rate     : 0.96

ğŸ’¡ Recommendation:
   âœ… READY FOR PRODUCTION - Excellent performance across all dimensions

ğŸ“ Full results saved to: eval_results.json
```

---

## ğŸ¯ Key Use Cases

### 1. Before Deploying New Model
```bash
# Evaluate your endpoint before production
python3 evals/pipeline_integration.py

# If score >= 0.85 and pass rate >= 0.90:
# â†’ Deploy to production âœ…

# If not:
# â†’ More training needed âŒ
```

### 2. Compare Two Models (A/B Testing)
```python
from evals import PipelineEvaluator

evaluator = PipelineEvaluator(project_id, region, bucket)

comparison = evaluator.compare_endpoints(
    endpoint_a_id="current-model",
    endpoint_b_id="new-model",
    test_cases=test_cases
)

print(f"Winner: {comparison['winner']}")
# Automatically choose better model!
```

### 3. Test Specific Response
```python
from evals import LLMJudge

judge = LLMJudge()

result = judge.evaluate_response(
    user_message="I need 2 tickets",
    agent_response="Great! Section B has 2 seats for $180 each. Total: $360",
    tools_used=["check_inventory"]
)

print(f"Overall: {result.overall_score:.2f}")
print(f"Feedback: {result.feedback}")
```

---

## ğŸ“ˆ Understanding Scores

### Overall Score Interpretation

| Score Range | Meaning | Action |
|-------------|---------|--------|
| 0.90 - 1.00 | Excellent | âœ… Deploy to production |
| 0.80 - 0.89 | Good | âœ… Deploy with monitoring |
| 0.70 - 0.79 | Acceptable | âš ï¸ Deploy to staging only |
| 0.60 - 0.69 | Poor | ğŸ”§ Needs improvement |
| Below 0.60 | Failing | âŒ Requires retraining |

### Dimension Scores

- **Helpfulness** < 0.80 â†’ Improve task completion
- **Accuracy** < 0.80 â†’ Fix hallucinations, verify data
- **Tool Usage** < 0.80 â†’ Better tool selection logic
- **Domain Expertise** < 0.80 â†’ Add domain knowledge

---

## ğŸ’° Cost Analysis

### Per Evaluation
- Domain checks: **$0.00** (instant)
- GPT-4 judge: **~$0.01** (~500ms)
- **Total: ~$0.01 per test**

### Realistic Usage
| Scenario | Tests | Cost | Time |
|----------|-------|------|------|
| Pre-deploy check | 10 tests | $0.10 | 5 seconds |
| Daily monitoring | 100 tests | $1.00 | 50 seconds |
| Full regression | 500 tests | $5.00 | 4 minutes |
| Monthly cost (daily monitoring) | 3,000 tests | ~$30 | Automated |

**Very affordable for the quality assurance value!**

---

## ğŸ”§ Configuration Options

### Use GPT-3.5 (Cheaper)
```bash
export EVAL_MODEL=gpt-3.5-turbo  # ~$0.001 per eval (10x cheaper)
```

### Use Vertex AI Gemini (No OpenAI needed)
```bash
export EVAL_PROVIDER=vertexai
# Uses your existing Google Cloud setup
```

### Adjust Thresholds
```bash
export MIN_EVAL_SCORE=0.85  # Higher for production
export MIN_PASS_RATE=0.90   # Require 90% pass rate
```

---

## ğŸ¯ Your Next Steps

### Immediate (Now)
1. **Set your OpenAI API key**:
   ```bash
   export OPENAI_API_KEY=sk-your-key-here
   ```

2. **Run first evaluation**:
   ```bash
   cd evals
   python3 eval_suite.py
   ```

3. **Review results** in `eval_results.json`

### Before Next Model Training
1. **Run evaluation** on current model
2. **Set baseline scores**
3. **After training**, re-evaluate
4. **Compare scores** - did it improve?
5. **Deploy** only if scores improved

### In Production
1. **Sample conversations daily** (100 random)
2. **Run evaluation** on sample
3. **Track scores over time**
4. **Alert if degradation** detected

---

## ğŸ“š Files You Can Use

### Ready to Run:
- âœ… `eval_suite.py` - Complete eval suite
- âœ… `llm_judge.py` - OpenAI judge standalone
- âœ… `domain_specific.py` - Domain checks standalone
- âœ… `pipeline_integration.py` - Pre-deployment automation

### Test Cases Included:
- âœ… 5 LLM test cases (TICKETING_EVAL_CASES)
- âœ… 4 domain test cases (DOMAIN_TEST_CASES)
- âœ… 10 standard test cases (create_standard_test_cases())

---

## âœ¨ Summary

**You now have a production-ready evaluation system that:**

âœ… Uses GPT-4 to grade responses like an expert
âœ… Validates domain logic with rules
âœ… Provides deployment recommendations
âœ… Costs ~$0.01 per evaluation
âœ… Runs in ~500ms per test
âœ… Includes 9+ pre-built test cases
âœ… Integrates with your training pipeline
âœ… Supports A/B testing

**Just add your OpenAI API key and run `python3 evals/eval_suite.py`!**

---

## ğŸ‰ What This Means

**Before deployment**, you can now automatically verify:
- Is the model helpful?
- Are prices calculated correctly?
- Does it use tools appropriately?
- Is the conversation flow natural?
- Does it follow ticketing policies?

**No more guessing** - you have quantitative scores and clear recommendations! ğŸš€
