{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Experiments - Qwen Messaging Agent\n",
        "\n",
        "This notebook provides tools for experimenting with different model configurations, hyperparameters, and evaluation metrics.\n",
        "\n",
        "## What you'll experiment with:\n",
        "- Model fine-tuning parameters\n",
        "- Hyperparameter optimization\n",
        "- A/B testing different model versions\n",
        "- Performance evaluation and comparison\n",
        "- Response quality assessment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries for model experimentation\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Dict, List, Any, Tuple\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Google Cloud imports\n",
        "from google.cloud import aiplatform\n",
        "from google.cloud import storage\n",
        "\n",
        "# Local imports\n",
        "import sys\n",
        "sys.path.append('../')\n",
        "from ml.ab_testing import ABTestingFramework\n",
        "from quality.quality_checks import QualityChecker\n",
        "\n",
        "print(\"🧪 Model experimentation libraries loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "PROJECT_ID = os.getenv('PROJECT_ID', 'your-project-id')\n",
        "REGION = os.getenv('REGION', 'us-central1')\n",
        "\n",
        "# Initialize Vertex AI\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "# Initialize quality checker\n",
        "quality_checker = QualityChecker()\n",
        "\n",
        "print(f\"🚀 Initialized experimentation environment for project: {PROJECT_ID}\")\n",
        "print(f\"📍 Region: {REGION}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test conversation scenarios for evaluation\n",
        "test_scenarios = [\n",
        "    {\n",
        "        \"scenario\": \"Ticket Purchase\",\n",
        "        \"message\": \"I need 2 tickets for tonight's Lakers game\",\n",
        "        \"expected_tools\": [\"get_event_info\", \"check_inventory\"],\n",
        "        \"expected_response_type\": \"helpful\"\n",
        "    },\n",
        "    {\n",
        "        \"scenario\": \"Seat Upgrade\",\n",
        "        \"message\": \"Can I upgrade my seats to a better section?\",\n",
        "        \"expected_tools\": [\"upgrade_tickets\"],\n",
        "        \"expected_response_type\": \"helpful\"\n",
        "    },\n",
        "    {\n",
        "        \"scenario\": \"General Inquiry\",\n",
        "        \"message\": \"What time does the game start?\",\n",
        "        \"expected_tools\": [\"get_event_info\"],\n",
        "        \"expected_response_type\": \"informative\"\n",
        "    },\n",
        "    {\n",
        "        \"scenario\": \"Complex Request\",\n",
        "        \"message\": \"I want to buy 4 tickets in section A, but if that's not available, I'll take section B, and I also want to know about parking options\",\n",
        "        \"expected_tools\": [\"get_event_info\", \"check_inventory\"],\n",
        "        \"expected_response_type\": \"comprehensive\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"📝 Loaded {len(test_scenarios)} test scenarios for evaluation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model evaluation function\n",
        "def evaluate_model_performance(endpoint_id: str, test_scenarios: List[Dict]) -> Dict[str, Any]:\n",
        "    \"\"\"Evaluate model performance on test scenarios\"\"\"\n",
        "    \n",
        "    results = {\n",
        "        \"endpoint_id\": endpoint_id,\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"scenarios\": [],\n",
        "        \"overall_metrics\": {}\n",
        "    }\n",
        "    \n",
        "    # Initialize endpoint\n",
        "    endpoint = aiplatform.Endpoint(f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_id}\")\n",
        "    \n",
        "    for scenario in test_scenarios:\n",
        "        print(f\"🧪 Testing scenario: {scenario['scenario']}\")\n",
        "        \n",
        "        # Get model response\n",
        "        start_time = datetime.now()\n",
        "        response = endpoint.predict(instances=[{\"messages\": [{\"role\": \"user\", \"content\": scenario[\"message\"]}]}])\n",
        "        end_time = datetime.now()\n",
        "        \n",
        "        response_time = (end_time - start_time).total_seconds() * 1000\n",
        "        model_response = response.predictions[0][\"response\"]\n",
        "        \n",
        "        # Quality assessment\n",
        "        quality_scores = quality_checker.assess_quality(\n",
        "            user_message=scenario[\"message\"],\n",
        "            agent_response=model_response\n",
        "        )\n",
        "        \n",
        "        scenario_result = {\n",
        "            \"scenario\": scenario[\"scenario\"],\n",
        "            \"message\": scenario[\"message\"],\n",
        "            \"response\": model_response,\n",
        "            \"response_time_ms\": response_time,\n",
        "            \"quality_scores\": quality_scores,\n",
        "            \"expected_tools\": scenario[\"expected_tools\"],\n",
        "            \"expected_response_type\": scenario[\"expected_response_type\"]\n",
        "        }\n",
        "        \n",
        "        results[\"scenarios\"].append(scenario_result)\n",
        "    \n",
        "    # Calculate overall metrics\n",
        "    response_times = [s[\"response_time_ms\"] for s in results[\"scenarios\"]]\n",
        "    quality_scores = [s[\"quality_scores\"][\"overall_score\"] for s in results[\"scenarios\"]]\n",
        "    \n",
        "    results[\"overall_metrics\"] = {\n",
        "        \"avg_response_time_ms\": np.mean(response_times),\n",
        "        \"avg_quality_score\": np.mean(quality_scores),\n",
        "        \"total_scenarios\": len(test_scenarios)\n",
        "    }\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"✅ Model evaluation function defined\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
